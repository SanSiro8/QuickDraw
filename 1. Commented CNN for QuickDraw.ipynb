{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0617ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier werden alle Bibliotheken impotriert welche f√ºr das Programm benutzt werden. \n",
    "# Dies wird in der ersten Zelle gemacht, damit man sie nicht mehrmals laden muss\n",
    "# Nach jeder Library werde ich noch den Link dazu angeben\n",
    "\n",
    "import numpy as np \n",
    "#https://numpy.org\n",
    "import torch \n",
    "# https://pytorch.org\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "import glob\n",
    "# https://docs.python.org/3/library/glob.html\n",
    "import os\n",
    "# https://docs.python.org/3/library/os.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "# https://scikit-learn.org/stable/\n",
    "from collections import Counter\n",
    "# https://docs.python.org/3/library/collections.html\n",
    "from itertools import zip_longest\n",
    "# https://docs.python.org/3/library/itertools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526ae8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "18\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.2212\n",
      "Epoch [2/150], Loss: 2.0894\n",
      "Epoch [3/150], Loss: 1.9825\n",
      "Epoch [4/150], Loss: 1.9991\n",
      "Epoch [5/150], Loss: 2.0197\n",
      "Epoch [6/150], Loss: 1.9040\n",
      "Epoch [7/150], Loss: 1.9860\n",
      "Epoch [8/150], Loss: 1.9338\n",
      "Epoch [9/150], Loss: 1.8420\n",
      "Epoch [10/150], Loss: 1.8570\n",
      "Epoch [11/150], Loss: 1.9230\n",
      "Epoch [12/150], Loss: 1.9045\n",
      "Epoch [13/150], Loss: 1.9388\n",
      "Epoch [14/150], Loss: 1.9467\n",
      "Epoch [15/150], Loss: 1.8992\n",
      "Epoch [16/150], Loss: 1.9181\n",
      "Epoch [17/150], Loss: 1.8802\n",
      "Epoch [18/150], Loss: 1.9296\n",
      "Epoch [19/150], Loss: 1.8619\n",
      "Epoch [20/150], Loss: 1.8626\n",
      "Epoch [21/150], Loss: 1.8520\n",
      "Epoch [22/150], Loss: 1.8889\n",
      "Epoch [23/150], Loss: 1.8285\n",
      "Epoch [24/150], Loss: 1.7966\n",
      "Epoch [25/150], Loss: 1.8710\n",
      "Epoch [26/150], Loss: 1.8736\n",
      "Epoch [27/150], Loss: 1.8283\n",
      "Epoch [28/150], Loss: 1.8672\n",
      "Epoch [29/150], Loss: 1.8199\n",
      "Epoch [30/150], Loss: 1.8409\n",
      "Epoch [31/150], Loss: 1.8093\n",
      "Epoch [32/150], Loss: 1.8918\n",
      "Epoch [33/150], Loss: 1.8785\n",
      "Epoch [34/150], Loss: 1.7810\n",
      "Epoch [35/150], Loss: 1.8236\n",
      "Epoch [36/150], Loss: 1.8449\n",
      "Epoch [37/150], Loss: 1.7983\n",
      "Epoch [38/150], Loss: 1.8549\n",
      "Epoch [39/150], Loss: 1.8753\n",
      "Epoch [40/150], Loss: 1.9173\n",
      "Epoch [41/150], Loss: 1.7927\n",
      "Epoch [42/150], Loss: 1.8989\n",
      "Epoch [43/150], Loss: 1.9179\n",
      "Epoch [44/150], Loss: 1.8343\n",
      "Epoch [45/150], Loss: 1.8499\n",
      "Epoch [46/150], Loss: 1.8376\n",
      "Epoch [47/150], Loss: 1.8560\n",
      "Epoch [48/150], Loss: 1.7979\n",
      "Epoch [49/150], Loss: 1.8163\n",
      "Epoch [50/150], Loss: 1.8636\n",
      "Epoch [51/150], Loss: 1.8211\n",
      "Epoch [52/150], Loss: 1.8814\n",
      "Epoch [53/150], Loss: 1.7903\n",
      "Epoch [54/150], Loss: 1.8996\n",
      "Epoch [55/150], Loss: 1.8020\n",
      "Epoch [56/150], Loss: 1.9429\n",
      "Epoch [57/150], Loss: 1.9044\n",
      "Epoch [58/150], Loss: 1.8268\n",
      "Epoch [59/150], Loss: 1.7443\n",
      "Epoch [60/150], Loss: 1.7197\n",
      "Epoch [61/150], Loss: 1.8954\n",
      "Epoch [62/150], Loss: 1.8494\n",
      "Epoch [63/150], Loss: 1.8713\n",
      "Epoch [64/150], Loss: 1.7876\n",
      "Epoch [65/150], Loss: 1.8527\n",
      "Epoch [66/150], Loss: 1.7925\n",
      "Epoch [67/150], Loss: 1.7422\n",
      "Epoch [68/150], Loss: 1.7633\n",
      "Epoch [69/150], Loss: 1.9163\n",
      "Epoch [70/150], Loss: 1.8092\n",
      "Epoch [71/150], Loss: 1.8376\n",
      "Epoch [72/150], Loss: 1.8275\n",
      "Epoch [73/150], Loss: 1.7215\n",
      "Epoch [74/150], Loss: 1.8548\n",
      "Epoch [75/150], Loss: 1.8298\n",
      "Epoch [76/150], Loss: 1.7900\n",
      "Epoch [77/150], Loss: 1.7128\n",
      "Epoch [78/150], Loss: 1.8408\n",
      "Epoch [79/150], Loss: 1.8079\n",
      "Epoch [80/150], Loss: 1.8465\n",
      "Epoch [81/150], Loss: 1.8444\n",
      "Epoch [82/150], Loss: 1.8687\n",
      "Epoch [83/150], Loss: 1.8636\n",
      "Epoch [84/150], Loss: 1.8351\n",
      "Epoch [85/150], Loss: 1.7772\n",
      "Epoch [86/150], Loss: 1.7418\n",
      "Epoch [87/150], Loss: 1.8507\n",
      "Epoch [88/150], Loss: 1.9256\n",
      "Epoch [89/150], Loss: 1.7860\n",
      "Epoch [90/150], Loss: 1.8155\n",
      "Epoch [91/150], Loss: 1.8727\n",
      "Epoch [92/150], Loss: 1.8832\n",
      "Epoch [93/150], Loss: 1.8773\n",
      "Epoch [94/150], Loss: 1.8646\n",
      "Epoch [95/150], Loss: 1.8369\n",
      "Epoch [96/150], Loss: 1.7861\n",
      "Epoch [97/150], Loss: 1.9318\n",
      "Epoch [98/150], Loss: 1.7914\n",
      "Epoch [99/150], Loss: 1.8548\n",
      "Epoch [100/150], Loss: 1.8321\n",
      "Epoch [101/150], Loss: 1.8529\n",
      "Epoch [102/150], Loss: 1.8877\n",
      "Epoch [103/150], Loss: 1.8059\n",
      "Epoch [104/150], Loss: 1.8310\n",
      "Epoch [105/150], Loss: 1.8131\n",
      "Epoch [106/150], Loss: 1.8288\n",
      "Epoch [107/150], Loss: 1.9441\n",
      "Epoch [108/150], Loss: 1.8597\n",
      "Epoch [109/150], Loss: 1.7860\n",
      "Epoch [110/150], Loss: 1.7720\n",
      "Epoch [111/150], Loss: 1.8067\n",
      "Epoch [112/150], Loss: 1.7442\n",
      "Epoch [113/150], Loss: 1.8177\n",
      "Epoch [114/150], Loss: 1.8097\n",
      "Epoch [115/150], Loss: 1.8906\n",
      "Epoch [116/150], Loss: 1.8050\n",
      "Epoch [117/150], Loss: 1.8471\n",
      "Epoch [118/150], Loss: 1.7804\n",
      "Epoch [119/150], Loss: 1.7554\n",
      "Epoch [120/150], Loss: 1.7488\n",
      "Epoch [121/150], Loss: 1.8217\n",
      "Epoch [122/150], Loss: 1.7814\n",
      "Epoch [123/150], Loss: 1.7985\n",
      "Epoch [124/150], Loss: 1.8611\n",
      "Epoch [125/150], Loss: 1.8313\n",
      "Epoch [126/150], Loss: 1.7944\n",
      "Epoch [127/150], Loss: 1.7756\n",
      "Epoch [128/150], Loss: 1.8617\n",
      "Epoch [129/150], Loss: 1.8692\n",
      "Epoch [130/150], Loss: 1.8358\n",
      "Epoch [131/150], Loss: 1.8538\n",
      "Epoch [132/150], Loss: 1.7975\n",
      "Epoch [133/150], Loss: 1.8504\n",
      "Epoch [134/150], Loss: 1.8796\n",
      "Epoch [135/150], Loss: 1.8064\n",
      "Epoch [136/150], Loss: 1.7624\n",
      "Epoch [137/150], Loss: 1.8489\n",
      "Epoch [138/150], Loss: 1.8081\n",
      "Epoch [139/150], Loss: 1.7918\n",
      "Epoch [140/150], Loss: 1.8468\n",
      "Epoch [141/150], Loss: 1.8204\n",
      "Epoch [142/150], Loss: 1.7257\n",
      "Epoch [143/150], Loss: 1.8531\n",
      "Epoch [144/150], Loss: 1.8742\n",
      "Epoch [145/150], Loss: 1.8204\n",
      "Epoch [146/150], Loss: 1.8350\n",
      "Epoch [147/150], Loss: 1.7969\n",
      "Epoch [148/150], Loss: 1.8375\n",
      "Epoch [149/150], Loss: 1.8104\n",
      "Epoch [150/150], Loss: 1.7884\n",
      "The Accuracy of the CNN is: 50.0\n",
      "*********\n",
      "19\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.2083\n",
      "Epoch [2/150], Loss: 2.0849\n",
      "Epoch [3/150], Loss: 2.1147\n",
      "Epoch [4/150], Loss: 1.9798\n",
      "Epoch [5/150], Loss: 2.0334\n",
      "Epoch [6/150], Loss: 1.9598\n",
      "Epoch [7/150], Loss: 1.9918\n",
      "Epoch [8/150], Loss: 2.0376\n",
      "Epoch [9/150], Loss: 1.8923\n",
      "Epoch [10/150], Loss: 1.9976\n",
      "Epoch [11/150], Loss: 1.8934\n",
      "Epoch [12/150], Loss: 1.9874\n",
      "Epoch [13/150], Loss: 1.8857\n",
      "Epoch [14/150], Loss: 1.8789\n",
      "Epoch [15/150], Loss: 1.9643\n",
      "Epoch [16/150], Loss: 1.9346\n",
      "Epoch [17/150], Loss: 1.9189\n",
      "Epoch [18/150], Loss: 1.9818\n",
      "Epoch [19/150], Loss: 1.9128\n",
      "Epoch [20/150], Loss: 1.8468\n",
      "Epoch [21/150], Loss: 1.9588\n",
      "Epoch [22/150], Loss: 2.0128\n",
      "Epoch [23/150], Loss: 1.9226\n",
      "Epoch [24/150], Loss: 1.9509\n",
      "Epoch [25/150], Loss: 1.9133\n",
      "Epoch [26/150], Loss: 1.9224\n",
      "Epoch [27/150], Loss: 1.8535\n",
      "Epoch [28/150], Loss: 1.8423\n",
      "Epoch [29/150], Loss: 1.9238\n",
      "Epoch [30/150], Loss: 1.8362\n",
      "Epoch [31/150], Loss: 1.9010\n",
      "Epoch [32/150], Loss: 1.9068\n",
      "Epoch [33/150], Loss: 1.9381\n",
      "Epoch [34/150], Loss: 1.8698\n",
      "Epoch [35/150], Loss: 1.8634\n",
      "Epoch [36/150], Loss: 1.8850\n",
      "Epoch [37/150], Loss: 1.8572\n",
      "Epoch [38/150], Loss: 1.8005\n",
      "Epoch [39/150], Loss: 1.8445\n",
      "Epoch [40/150], Loss: 1.9184\n",
      "Epoch [41/150], Loss: 1.9111\n",
      "Epoch [42/150], Loss: 1.8258\n",
      "Epoch [43/150], Loss: 1.8823\n",
      "Epoch [44/150], Loss: 1.8450\n",
      "Epoch [45/150], Loss: 1.8181\n",
      "Epoch [46/150], Loss: 1.8177\n",
      "Epoch [47/150], Loss: 1.8056\n",
      "Epoch [48/150], Loss: 1.8443\n",
      "Epoch [49/150], Loss: 1.8663\n",
      "Epoch [50/150], Loss: 1.8249\n",
      "Epoch [51/150], Loss: 1.8648\n",
      "Epoch [52/150], Loss: 1.7954\n",
      "Epoch [53/150], Loss: 1.8481\n",
      "Epoch [54/150], Loss: 1.8890\n",
      "Epoch [55/150], Loss: 1.7718\n",
      "Epoch [56/150], Loss: 1.9047\n",
      "Epoch [57/150], Loss: 1.7920\n",
      "Epoch [58/150], Loss: 1.8341\n",
      "Epoch [59/150], Loss: 1.8642\n",
      "Epoch [60/150], Loss: 1.7875\n",
      "Epoch [61/150], Loss: 1.8162\n",
      "Epoch [62/150], Loss: 1.8132\n",
      "Epoch [63/150], Loss: 1.7657\n",
      "Epoch [64/150], Loss: 1.7775\n",
      "Epoch [65/150], Loss: 1.8528\n",
      "Epoch [66/150], Loss: 1.7252\n",
      "Epoch [67/150], Loss: 1.7817\n",
      "Epoch [68/150], Loss: 1.7857\n",
      "Epoch [69/150], Loss: 1.8127\n",
      "Epoch [70/150], Loss: 1.7156\n",
      "Epoch [71/150], Loss: 1.7740\n",
      "Epoch [72/150], Loss: 1.8225\n",
      "Epoch [73/150], Loss: 1.8004\n",
      "Epoch [74/150], Loss: 1.7902\n",
      "Epoch [75/150], Loss: 1.7963\n",
      "Epoch [76/150], Loss: 1.7082\n",
      "Epoch [77/150], Loss: 1.7649\n",
      "Epoch [78/150], Loss: 1.6655\n",
      "Epoch [79/150], Loss: 1.8056\n",
      "Epoch [80/150], Loss: 1.8039\n",
      "Epoch [81/150], Loss: 1.8656\n",
      "Epoch [82/150], Loss: 1.7881\n",
      "Epoch [83/150], Loss: 1.7820\n",
      "Epoch [84/150], Loss: 1.8136\n",
      "Epoch [85/150], Loss: 1.8139\n",
      "Epoch [86/150], Loss: 1.7779\n",
      "Epoch [87/150], Loss: 1.7636\n",
      "Epoch [88/150], Loss: 1.6788\n",
      "Epoch [89/150], Loss: 1.7842\n",
      "Epoch [90/150], Loss: 1.7202\n",
      "Epoch [91/150], Loss: 1.6851\n",
      "Epoch [92/150], Loss: 1.7813\n",
      "Epoch [93/150], Loss: 1.6946\n",
      "Epoch [94/150], Loss: 1.6948\n",
      "Epoch [95/150], Loss: 1.7990\n",
      "Epoch [96/150], Loss: 1.7084\n",
      "Epoch [97/150], Loss: 1.7317\n",
      "Epoch [98/150], Loss: 1.7326\n",
      "Epoch [99/150], Loss: 1.6949\n",
      "Epoch [100/150], Loss: 1.6764\n",
      "Epoch [101/150], Loss: 1.8043\n",
      "Epoch [102/150], Loss: 1.7377\n",
      "Epoch [103/150], Loss: 1.7125\n",
      "Epoch [104/150], Loss: 1.7231\n",
      "Epoch [105/150], Loss: 1.7017\n",
      "Epoch [106/150], Loss: 1.6849\n",
      "Epoch [107/150], Loss: 1.7041\n",
      "Epoch [108/150], Loss: 1.7057\n",
      "Epoch [109/150], Loss: 1.6732\n",
      "Epoch [110/150], Loss: 1.7869\n",
      "Epoch [111/150], Loss: 1.7288\n",
      "Epoch [112/150], Loss: 1.6916\n",
      "Epoch [113/150], Loss: 1.6871\n",
      "Epoch [114/150], Loss: 1.7671\n",
      "Epoch [115/150], Loss: 1.8139\n",
      "Epoch [116/150], Loss: 1.7231\n",
      "Epoch [117/150], Loss: 1.6927\n",
      "Epoch [118/150], Loss: 1.5948\n",
      "Epoch [119/150], Loss: 1.6919\n",
      "Epoch [120/150], Loss: 1.7166\n",
      "Epoch [121/150], Loss: 1.7326\n",
      "Epoch [122/150], Loss: 1.6846\n",
      "Epoch [123/150], Loss: 1.6733\n",
      "Epoch [124/150], Loss: 1.6358\n",
      "Epoch [125/150], Loss: 1.7093\n",
      "Epoch [126/150], Loss: 1.7480\n",
      "Epoch [127/150], Loss: 1.6316\n",
      "Epoch [128/150], Loss: 1.7658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [129/150], Loss: 1.6679\n",
      "Epoch [130/150], Loss: 1.6876\n",
      "Epoch [131/150], Loss: 1.6919\n",
      "Epoch [132/150], Loss: 1.6725\n",
      "Epoch [133/150], Loss: 1.6373\n",
      "Epoch [134/150], Loss: 1.6753\n",
      "Epoch [135/150], Loss: 1.7261\n",
      "Epoch [136/150], Loss: 1.7033\n",
      "Epoch [137/150], Loss: 1.6987\n",
      "Epoch [138/150], Loss: 1.6643\n",
      "Epoch [139/150], Loss: 1.6089\n",
      "Epoch [140/150], Loss: 1.7704\n",
      "Epoch [141/150], Loss: 1.6828\n",
      "Epoch [142/150], Loss: 1.6917\n",
      "Epoch [143/150], Loss: 1.5522\n",
      "Epoch [144/150], Loss: 1.6649\n",
      "Epoch [145/150], Loss: 1.7156\n",
      "Epoch [146/150], Loss: 1.6049\n",
      "Epoch [147/150], Loss: 1.6658\n",
      "Epoch [148/150], Loss: 1.6236\n",
      "Epoch [149/150], Loss: 1.6820\n",
      "Epoch [150/150], Loss: 1.7886\n",
      "The Accuracy of the CNN is: 90.0\n",
      "*********\n",
      "20\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.0788\n",
      "Epoch [2/150], Loss: 2.0731\n",
      "Epoch [3/150], Loss: 2.0099\n",
      "Epoch [4/150], Loss: 1.9952\n",
      "Epoch [5/150], Loss: 2.1189\n",
      "Epoch [6/150], Loss: 1.9225\n",
      "Epoch [7/150], Loss: 1.9941\n",
      "Epoch [8/150], Loss: 2.0498\n",
      "Epoch [9/150], Loss: 1.9164\n",
      "Epoch [10/150], Loss: 2.0705\n",
      "Epoch [11/150], Loss: 1.9794\n",
      "Epoch [12/150], Loss: 2.0601\n",
      "Epoch [13/150], Loss: 2.0048\n",
      "Epoch [14/150], Loss: 2.0575\n",
      "Epoch [15/150], Loss: 1.9523\n",
      "Epoch [16/150], Loss: 2.0035\n",
      "Epoch [17/150], Loss: 1.9589\n",
      "Epoch [18/150], Loss: 1.8643\n",
      "Epoch [19/150], Loss: 1.9772\n",
      "Epoch [20/150], Loss: 2.0665\n",
      "Epoch [21/150], Loss: 2.0453\n",
      "Epoch [22/150], Loss: 1.8800\n",
      "Epoch [23/150], Loss: 1.9632\n",
      "Epoch [24/150], Loss: 2.0014\n",
      "Epoch [25/150], Loss: 1.9484\n",
      "Epoch [26/150], Loss: 1.8999\n",
      "Epoch [27/150], Loss: 1.9437\n",
      "Epoch [28/150], Loss: 1.9459\n",
      "Epoch [29/150], Loss: 1.9324\n",
      "Epoch [30/150], Loss: 1.9220\n",
      "Epoch [31/150], Loss: 1.8702\n",
      "Epoch [32/150], Loss: 1.9884\n",
      "Epoch [33/150], Loss: 1.9100\n",
      "Epoch [34/150], Loss: 2.0003\n",
      "Epoch [35/150], Loss: 1.9724\n",
      "Epoch [36/150], Loss: 2.0111\n",
      "Epoch [37/150], Loss: 2.0025\n",
      "Epoch [38/150], Loss: 1.9925\n",
      "Epoch [39/150], Loss: 1.9856\n",
      "Epoch [40/150], Loss: 2.0536\n",
      "Epoch [41/150], Loss: 1.9042\n",
      "Epoch [42/150], Loss: 1.8877\n",
      "Epoch [43/150], Loss: 1.9524\n",
      "Epoch [44/150], Loss: 1.9093\n",
      "Epoch [45/150], Loss: 1.9835\n",
      "Epoch [46/150], Loss: 1.9935\n",
      "Epoch [47/150], Loss: 2.0218\n",
      "Epoch [48/150], Loss: 1.8608\n",
      "Epoch [49/150], Loss: 1.9864\n",
      "Epoch [50/150], Loss: 1.9925\n",
      "Epoch [51/150], Loss: 2.0134\n",
      "Epoch [52/150], Loss: 1.9382\n",
      "Epoch [53/150], Loss: 1.8918\n",
      "Epoch [54/150], Loss: 1.9089\n",
      "Epoch [55/150], Loss: 1.9983\n",
      "Epoch [56/150], Loss: 1.8865\n",
      "Epoch [57/150], Loss: 1.9674\n",
      "Epoch [58/150], Loss: 1.8218\n",
      "Epoch [59/150], Loss: 1.8882\n",
      "Epoch [60/150], Loss: 1.9234\n",
      "Epoch [61/150], Loss: 1.9937\n",
      "Epoch [62/150], Loss: 1.9086\n",
      "Epoch [63/150], Loss: 1.9472\n",
      "Epoch [64/150], Loss: 1.9178\n",
      "Epoch [65/150], Loss: 1.9249\n",
      "Epoch [66/150], Loss: 2.0086\n",
      "Epoch [67/150], Loss: 1.9092\n",
      "Epoch [68/150], Loss: 1.9313\n",
      "Epoch [69/150], Loss: 1.8277\n",
      "Epoch [70/150], Loss: 1.9352\n",
      "Epoch [71/150], Loss: 1.8269\n",
      "Epoch [72/150], Loss: 2.0321\n",
      "Epoch [73/150], Loss: 1.9791\n",
      "Epoch [74/150], Loss: 1.9583\n",
      "Epoch [75/150], Loss: 1.9968\n",
      "Epoch [76/150], Loss: 1.9200\n",
      "Epoch [77/150], Loss: 1.8960\n",
      "Epoch [78/150], Loss: 1.8712\n",
      "Epoch [79/150], Loss: 2.0170\n",
      "Epoch [80/150], Loss: 1.9895\n",
      "Epoch [81/150], Loss: 1.8544\n",
      "Epoch [82/150], Loss: 1.8078\n",
      "Epoch [83/150], Loss: 1.9884\n",
      "Epoch [84/150], Loss: 1.9309\n",
      "Epoch [85/150], Loss: 1.8516\n",
      "Epoch [86/150], Loss: 1.9377\n",
      "Epoch [87/150], Loss: 1.9231\n",
      "Epoch [88/150], Loss: 1.9933\n",
      "Epoch [89/150], Loss: 1.9713\n",
      "Epoch [90/150], Loss: 1.9534\n",
      "Epoch [91/150], Loss: 1.9296\n",
      "Epoch [92/150], Loss: 1.9307\n",
      "Epoch [93/150], Loss: 1.9470\n",
      "Epoch [94/150], Loss: 1.8299\n",
      "Epoch [95/150], Loss: 1.9739\n",
      "Epoch [96/150], Loss: 1.9208\n",
      "Epoch [97/150], Loss: 1.8699\n",
      "Epoch [98/150], Loss: 1.9002\n",
      "Epoch [99/150], Loss: 1.9618\n",
      "Epoch [100/150], Loss: 1.8575\n",
      "Epoch [101/150], Loss: 1.9219\n",
      "Epoch [102/150], Loss: 1.9209\n",
      "Epoch [103/150], Loss: 1.8442\n",
      "Epoch [104/150], Loss: 1.9381\n",
      "Epoch [105/150], Loss: 1.9134\n",
      "Epoch [106/150], Loss: 1.9188\n",
      "Epoch [107/150], Loss: 1.8634\n",
      "Epoch [108/150], Loss: 2.0024\n",
      "Epoch [109/150], Loss: 1.9703\n",
      "Epoch [110/150], Loss: 1.8385\n",
      "Epoch [111/150], Loss: 1.9586\n",
      "Epoch [112/150], Loss: 1.9165\n",
      "Epoch [113/150], Loss: 1.8683\n",
      "Epoch [114/150], Loss: 1.8799\n",
      "Epoch [115/150], Loss: 1.8710\n",
      "Epoch [116/150], Loss: 1.9199\n",
      "Epoch [117/150], Loss: 1.8041\n",
      "Epoch [118/150], Loss: 2.0140\n",
      "Epoch [119/150], Loss: 1.9295\n",
      "Epoch [120/150], Loss: 1.8837\n",
      "Epoch [121/150], Loss: 1.9589\n",
      "Epoch [122/150], Loss: 1.9178\n",
      "Epoch [123/150], Loss: 1.8537\n",
      "Epoch [124/150], Loss: 1.9148\n",
      "Epoch [125/150], Loss: 1.8809\n",
      "Epoch [126/150], Loss: 1.8982\n",
      "Epoch [127/150], Loss: 1.8463\n",
      "Epoch [128/150], Loss: 1.9421\n",
      "Epoch [129/150], Loss: 1.8934\n",
      "Epoch [130/150], Loss: 1.9040\n",
      "Epoch [131/150], Loss: 1.9365\n",
      "Epoch [132/150], Loss: 1.8764\n",
      "Epoch [133/150], Loss: 1.8838\n",
      "Epoch [134/150], Loss: 1.9729\n",
      "Epoch [135/150], Loss: 1.8709\n",
      "Epoch [136/150], Loss: 1.8437\n",
      "Epoch [137/150], Loss: 1.9095\n",
      "Epoch [138/150], Loss: 1.9949\n",
      "Epoch [139/150], Loss: 2.0473\n",
      "Epoch [140/150], Loss: 1.9229\n",
      "Epoch [141/150], Loss: 1.8589\n",
      "Epoch [142/150], Loss: 1.9224\n",
      "Epoch [143/150], Loss: 1.9043\n",
      "Epoch [144/150], Loss: 1.9714\n",
      "Epoch [145/150], Loss: 1.8267\n",
      "Epoch [146/150], Loss: 1.9184\n",
      "Epoch [147/150], Loss: 1.9102\n",
      "Epoch [148/150], Loss: 1.9213\n",
      "Epoch [149/150], Loss: 1.8256\n",
      "Epoch [150/150], Loss: 1.8266\n",
      "The Accuracy of the CNN is: 45.0\n",
      "*********\n",
      "21\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.0931\n",
      "Epoch [2/150], Loss: 2.0392\n",
      "Epoch [3/150], Loss: 2.1056\n",
      "Epoch [4/150], Loss: 1.9727\n",
      "Epoch [5/150], Loss: 1.8837\n",
      "Epoch [6/150], Loss: 1.9459\n",
      "Epoch [7/150], Loss: 1.8591\n",
      "Epoch [8/150], Loss: 1.9757\n",
      "Epoch [9/150], Loss: 1.9306\n",
      "Epoch [10/150], Loss: 1.8569\n",
      "Epoch [11/150], Loss: 1.9002\n",
      "Epoch [12/150], Loss: 1.9808\n",
      "Epoch [13/150], Loss: 1.8769\n",
      "Epoch [14/150], Loss: 1.9510\n",
      "Epoch [15/150], Loss: 1.9394\n",
      "Epoch [16/150], Loss: 1.9402\n",
      "Epoch [17/150], Loss: 1.9794\n",
      "Epoch [18/150], Loss: 1.8646\n",
      "Epoch [19/150], Loss: 1.8965\n",
      "Epoch [20/150], Loss: 1.8492\n",
      "Epoch [21/150], Loss: 1.9252\n",
      "Epoch [22/150], Loss: 1.9012\n",
      "Epoch [23/150], Loss: 1.8838\n",
      "Epoch [24/150], Loss: 1.7968\n",
      "Epoch [25/150], Loss: 1.8278\n",
      "Epoch [26/150], Loss: 1.8135\n",
      "Epoch [27/150], Loss: 1.8857\n",
      "Epoch [28/150], Loss: 1.9318\n",
      "Epoch [29/150], Loss: 1.8415\n",
      "Epoch [30/150], Loss: 1.9336\n",
      "Epoch [31/150], Loss: 1.9418\n",
      "Epoch [32/150], Loss: 1.9130\n",
      "Epoch [33/150], Loss: 1.9040\n",
      "Epoch [34/150], Loss: 1.8803\n",
      "Epoch [35/150], Loss: 1.8884\n",
      "Epoch [36/150], Loss: 1.8735\n",
      "Epoch [37/150], Loss: 1.8984\n",
      "Epoch [38/150], Loss: 1.8426\n",
      "Epoch [39/150], Loss: 1.8565\n",
      "Epoch [40/150], Loss: 1.9532\n",
      "Epoch [41/150], Loss: 1.8255\n",
      "Epoch [42/150], Loss: 1.8371\n",
      "Epoch [43/150], Loss: 1.8708\n",
      "Epoch [44/150], Loss: 1.8103\n",
      "Epoch [45/150], Loss: 1.9215\n",
      "Epoch [46/150], Loss: 1.9290\n",
      "Epoch [47/150], Loss: 1.8513\n",
      "Epoch [48/150], Loss: 1.9097\n",
      "Epoch [49/150], Loss: 1.9169\n",
      "Epoch [50/150], Loss: 1.8653\n",
      "Epoch [51/150], Loss: 1.9420\n",
      "Epoch [52/150], Loss: 1.9371\n",
      "Epoch [53/150], Loss: 1.9918\n",
      "Epoch [54/150], Loss: 1.7508\n",
      "Epoch [55/150], Loss: 1.8877\n",
      "Epoch [56/150], Loss: 1.8884\n",
      "Epoch [57/150], Loss: 1.9125\n",
      "Epoch [58/150], Loss: 1.8358\n",
      "Epoch [59/150], Loss: 1.8895\n",
      "Epoch [60/150], Loss: 1.9448\n",
      "Epoch [61/150], Loss: 1.9369\n",
      "Epoch [62/150], Loss: 1.9313\n",
      "Epoch [63/150], Loss: 1.9382\n",
      "Epoch [64/150], Loss: 1.8355\n",
      "Epoch [65/150], Loss: 1.8678\n",
      "Epoch [66/150], Loss: 1.9239\n",
      "Epoch [67/150], Loss: 1.8900\n",
      "Epoch [68/150], Loss: 1.8912\n",
      "Epoch [69/150], Loss: 1.8693\n",
      "Epoch [70/150], Loss: 1.9165\n",
      "Epoch [71/150], Loss: 1.8599\n",
      "Epoch [72/150], Loss: 1.9387\n",
      "Epoch [73/150], Loss: 1.8430\n",
      "Epoch [74/150], Loss: 1.8828\n",
      "Epoch [75/150], Loss: 1.8755\n",
      "Epoch [76/150], Loss: 1.9231\n",
      "Epoch [77/150], Loss: 1.8727\n",
      "Epoch [78/150], Loss: 1.8564\n",
      "Epoch [79/150], Loss: 1.8713\n",
      "Epoch [80/150], Loss: 1.9549\n",
      "Epoch [81/150], Loss: 1.8930\n",
      "Epoch [82/150], Loss: 1.8791\n",
      "Epoch [83/150], Loss: 1.9009\n",
      "Epoch [84/150], Loss: 1.8465\n",
      "Epoch [85/150], Loss: 1.8809\n",
      "Epoch [86/150], Loss: 1.9516\n",
      "Epoch [87/150], Loss: 1.8597\n",
      "Epoch [88/150], Loss: 1.9429\n",
      "Epoch [89/150], Loss: 1.8655\n",
      "Epoch [90/150], Loss: 1.8435\n",
      "Epoch [91/150], Loss: 1.8528\n",
      "Epoch [92/150], Loss: 1.9308\n",
      "Epoch [93/150], Loss: 1.7252\n",
      "Epoch [94/150], Loss: 1.9057\n",
      "Epoch [95/150], Loss: 1.8059\n",
      "Epoch [96/150], Loss: 1.9154\n",
      "Epoch [97/150], Loss: 1.9082\n",
      "Epoch [98/150], Loss: 1.9868\n",
      "Epoch [99/150], Loss: 1.8505\n",
      "Epoch [100/150], Loss: 1.9160\n",
      "Epoch [101/150], Loss: 1.9468\n",
      "Epoch [102/150], Loss: 1.8778\n",
      "Epoch [103/150], Loss: 1.9449\n",
      "Epoch [104/150], Loss: 1.9682\n",
      "Epoch [105/150], Loss: 1.8942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [106/150], Loss: 1.7429\n",
      "Epoch [107/150], Loss: 1.8625\n",
      "Epoch [108/150], Loss: 1.8184\n",
      "Epoch [109/150], Loss: 1.8855\n",
      "Epoch [110/150], Loss: 1.8776\n",
      "Epoch [111/150], Loss: 1.8111\n",
      "Epoch [112/150], Loss: 1.8131\n",
      "Epoch [113/150], Loss: 1.9298\n",
      "Epoch [114/150], Loss: 1.8346\n",
      "Epoch [115/150], Loss: 1.8872\n",
      "Epoch [116/150], Loss: 1.9533\n",
      "Epoch [117/150], Loss: 1.9011\n",
      "Epoch [118/150], Loss: 1.8985\n",
      "Epoch [119/150], Loss: 1.8261\n",
      "Epoch [120/150], Loss: 1.8799\n",
      "Epoch [121/150], Loss: 1.8989\n",
      "Epoch [122/150], Loss: 1.9373\n",
      "Epoch [123/150], Loss: 1.8670\n",
      "Epoch [124/150], Loss: 1.8616\n",
      "Epoch [125/150], Loss: 1.8820\n",
      "Epoch [126/150], Loss: 1.9909\n",
      "Epoch [127/150], Loss: 1.8426\n",
      "Epoch [128/150], Loss: 1.9103\n",
      "Epoch [129/150], Loss: 1.9027\n",
      "Epoch [130/150], Loss: 1.8828\n",
      "Epoch [131/150], Loss: 1.8960\n",
      "Epoch [132/150], Loss: 1.8713\n",
      "Epoch [133/150], Loss: 1.8056\n",
      "Epoch [134/150], Loss: 1.8921\n",
      "Epoch [135/150], Loss: 1.8005\n",
      "Epoch [136/150], Loss: 1.8980\n",
      "Epoch [137/150], Loss: 1.8623\n",
      "Epoch [138/150], Loss: 1.8775\n",
      "Epoch [139/150], Loss: 1.8749\n",
      "Epoch [140/150], Loss: 1.7715\n",
      "Epoch [141/150], Loss: 1.8796\n",
      "Epoch [142/150], Loss: 1.8308\n",
      "Epoch [143/150], Loss: 1.9824\n",
      "Epoch [144/150], Loss: 1.8024\n",
      "Epoch [145/150], Loss: 1.8717\n",
      "Epoch [146/150], Loss: 1.9266\n",
      "Epoch [147/150], Loss: 1.8492\n",
      "Epoch [148/150], Loss: 1.8260\n",
      "Epoch [149/150], Loss: 1.8460\n",
      "Epoch [150/150], Loss: 1.9402\n",
      "The Accuracy of the CNN is: 70.0\n",
      "*********\n",
      "22\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.2946\n",
      "Epoch [2/150], Loss: 2.2086\n",
      "Epoch [3/150], Loss: 2.1738\n",
      "Epoch [4/150], Loss: 2.1436\n",
      "Epoch [5/150], Loss: 1.9665\n",
      "Epoch [6/150], Loss: 1.9829\n",
      "Epoch [7/150], Loss: 2.0062\n",
      "Epoch [8/150], Loss: 1.9868\n",
      "Epoch [9/150], Loss: 2.0879\n",
      "Epoch [10/150], Loss: 1.9085\n",
      "Epoch [11/150], Loss: 1.9068\n",
      "Epoch [12/150], Loss: 1.9241\n",
      "Epoch [13/150], Loss: 1.8438\n",
      "Epoch [14/150], Loss: 1.8808\n",
      "Epoch [15/150], Loss: 1.8688\n",
      "Epoch [16/150], Loss: 1.9101\n",
      "Epoch [17/150], Loss: 1.8421\n",
      "Epoch [18/150], Loss: 1.9871\n",
      "Epoch [19/150], Loss: 1.9877\n",
      "Epoch [20/150], Loss: 1.8173\n",
      "Epoch [21/150], Loss: 1.8849\n",
      "Epoch [22/150], Loss: 1.9307\n",
      "Epoch [23/150], Loss: 1.8129\n",
      "Epoch [24/150], Loss: 1.8746\n",
      "Epoch [25/150], Loss: 1.7906\n",
      "Epoch [26/150], Loss: 1.8520\n",
      "Epoch [27/150], Loss: 1.8225\n",
      "Epoch [28/150], Loss: 1.8466\n",
      "Epoch [29/150], Loss: 1.8370\n",
      "Epoch [30/150], Loss: 1.7987\n",
      "Epoch [31/150], Loss: 1.8372\n",
      "Epoch [32/150], Loss: 1.8015\n",
      "Epoch [33/150], Loss: 1.9101\n",
      "Epoch [34/150], Loss: 1.7414\n",
      "Epoch [35/150], Loss: 1.8355\n",
      "Epoch [36/150], Loss: 1.8285\n",
      "Epoch [37/150], Loss: 1.8406\n",
      "Epoch [38/150], Loss: 1.7508\n",
      "Epoch [39/150], Loss: 1.7728\n",
      "Epoch [40/150], Loss: 1.8308\n",
      "Epoch [41/150], Loss: 1.9319\n",
      "Epoch [42/150], Loss: 1.7370\n",
      "Epoch [43/150], Loss: 1.8057\n",
      "Epoch [44/150], Loss: 1.7291\n",
      "Epoch [45/150], Loss: 1.7229\n",
      "Epoch [46/150], Loss: 1.7595\n",
      "Epoch [47/150], Loss: 1.8303\n",
      "Epoch [48/150], Loss: 1.8977\n",
      "Epoch [49/150], Loss: 1.7933\n",
      "Epoch [50/150], Loss: 1.8021\n",
      "Epoch [51/150], Loss: 1.7606\n",
      "Epoch [52/150], Loss: 1.7255\n",
      "Epoch [53/150], Loss: 1.9090\n",
      "Epoch [54/150], Loss: 1.7641\n",
      "Epoch [55/150], Loss: 1.7456\n",
      "Epoch [56/150], Loss: 1.7097\n",
      "Epoch [57/150], Loss: 1.7533\n",
      "Epoch [58/150], Loss: 1.7815\n",
      "Epoch [59/150], Loss: 1.6849\n",
      "Epoch [60/150], Loss: 1.8057\n",
      "Epoch [61/150], Loss: 1.8362\n",
      "Epoch [62/150], Loss: 1.7341\n",
      "Epoch [63/150], Loss: 1.6863\n",
      "Epoch [64/150], Loss: 1.7657\n",
      "Epoch [65/150], Loss: 1.7200\n",
      "Epoch [66/150], Loss: 1.8012\n",
      "Epoch [67/150], Loss: 1.7765\n",
      "Epoch [68/150], Loss: 1.7194\n",
      "Epoch [69/150], Loss: 1.8471\n",
      "Epoch [70/150], Loss: 1.6887\n",
      "Epoch [71/150], Loss: 1.7605\n",
      "Epoch [72/150], Loss: 1.7838\n",
      "Epoch [73/150], Loss: 1.6885\n",
      "Epoch [74/150], Loss: 1.7649\n",
      "Epoch [75/150], Loss: 1.7689\n",
      "Epoch [76/150], Loss: 1.8642\n",
      "Epoch [77/150], Loss: 1.7329\n",
      "Epoch [78/150], Loss: 1.7480\n",
      "Epoch [79/150], Loss: 1.7154\n",
      "Epoch [80/150], Loss: 1.7667\n",
      "Epoch [81/150], Loss: 1.7301\n",
      "Epoch [82/150], Loss: 1.7554\n",
      "Epoch [83/150], Loss: 1.7222\n",
      "Epoch [84/150], Loss: 1.7016\n",
      "Epoch [85/150], Loss: 1.7552\n",
      "Epoch [86/150], Loss: 1.6808\n",
      "Epoch [87/150], Loss: 1.7457\n",
      "Epoch [88/150], Loss: 1.7997\n",
      "Epoch [89/150], Loss: 1.7037\n",
      "Epoch [90/150], Loss: 1.7176\n",
      "Epoch [91/150], Loss: 1.7581\n",
      "Epoch [92/150], Loss: 1.7075\n",
      "Epoch [93/150], Loss: 1.8255\n",
      "Epoch [94/150], Loss: 1.7050\n",
      "Epoch [95/150], Loss: 1.7540\n",
      "Epoch [96/150], Loss: 1.7371\n",
      "Epoch [97/150], Loss: 1.7243\n",
      "Epoch [98/150], Loss: 1.7048\n",
      "Epoch [99/150], Loss: 1.6531\n",
      "Epoch [100/150], Loss: 1.6697\n",
      "Epoch [101/150], Loss: 1.7487\n",
      "Epoch [102/150], Loss: 1.8212\n",
      "Epoch [103/150], Loss: 1.6899\n",
      "Epoch [104/150], Loss: 1.6842\n",
      "Epoch [105/150], Loss: 1.7954\n",
      "Epoch [106/150], Loss: 1.7384\n",
      "Epoch [107/150], Loss: 1.6971\n",
      "Epoch [108/150], Loss: 1.7186\n",
      "Epoch [109/150], Loss: 1.7124\n",
      "Epoch [110/150], Loss: 1.7820\n",
      "Epoch [111/150], Loss: 1.7662\n",
      "Epoch [112/150], Loss: 1.6978\n",
      "Epoch [113/150], Loss: 1.6987\n",
      "Epoch [114/150], Loss: 1.7372\n",
      "Epoch [115/150], Loss: 1.7380\n",
      "Epoch [116/150], Loss: 1.7280\n",
      "Epoch [117/150], Loss: 1.7689\n",
      "Epoch [118/150], Loss: 1.6999\n",
      "Epoch [119/150], Loss: 1.6570\n",
      "Epoch [120/150], Loss: 1.6769\n",
      "Epoch [121/150], Loss: 1.7052\n",
      "Epoch [122/150], Loss: 1.7070\n",
      "Epoch [123/150], Loss: 1.7663\n",
      "Epoch [124/150], Loss: 1.6903\n",
      "Epoch [125/150], Loss: 1.7187\n",
      "Epoch [126/150], Loss: 1.7083\n",
      "Epoch [127/150], Loss: 1.6945\n",
      "Epoch [128/150], Loss: 1.7680\n",
      "Epoch [129/150], Loss: 1.6587\n",
      "Epoch [130/150], Loss: 1.7013\n",
      "Epoch [131/150], Loss: 1.7611\n",
      "Epoch [132/150], Loss: 1.7975\n",
      "Epoch [133/150], Loss: 1.7194\n",
      "Epoch [134/150], Loss: 1.7522\n",
      "Epoch [135/150], Loss: 1.7476\n",
      "Epoch [136/150], Loss: 1.7357\n",
      "Epoch [137/150], Loss: 1.7191\n",
      "Epoch [138/150], Loss: 1.7301\n",
      "Epoch [139/150], Loss: 1.6313\n",
      "Epoch [140/150], Loss: 1.7097\n",
      "Epoch [141/150], Loss: 1.7892\n",
      "Epoch [142/150], Loss: 1.7230\n",
      "Epoch [143/150], Loss: 1.6893\n",
      "Epoch [144/150], Loss: 1.6524\n",
      "Epoch [145/150], Loss: 1.6266\n",
      "Epoch [146/150], Loss: 1.7034\n",
      "Epoch [147/150], Loss: 1.6631\n",
      "Epoch [148/150], Loss: 1.7806\n",
      "Epoch [149/150], Loss: 1.6580\n",
      "Epoch [150/150], Loss: 1.6928\n",
      "The Accuracy of the CNN is: 85.0\n",
      "*********\n",
      "23\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.0463\n",
      "Epoch [2/150], Loss: 2.0900\n",
      "Epoch [3/150], Loss: 2.0045\n",
      "Epoch [4/150], Loss: 1.9919\n",
      "Epoch [5/150], Loss: 2.0472\n",
      "Epoch [6/150], Loss: 1.9935\n",
      "Epoch [7/150], Loss: 1.7779\n",
      "Epoch [8/150], Loss: 1.9065\n",
      "Epoch [9/150], Loss: 1.8671\n",
      "Epoch [10/150], Loss: 1.8011\n",
      "Epoch [11/150], Loss: 1.8560\n",
      "Epoch [12/150], Loss: 1.8832\n",
      "Epoch [13/150], Loss: 1.8407\n",
      "Epoch [14/150], Loss: 1.8351\n",
      "Epoch [15/150], Loss: 1.8719\n",
      "Epoch [16/150], Loss: 1.8351\n",
      "Epoch [17/150], Loss: 1.8347\n",
      "Epoch [18/150], Loss: 1.8255\n",
      "Epoch [19/150], Loss: 1.8606\n",
      "Epoch [20/150], Loss: 1.7538\n",
      "Epoch [21/150], Loss: 1.8811\n",
      "Epoch [22/150], Loss: 1.7655\n",
      "Epoch [23/150], Loss: 1.7652\n",
      "Epoch [24/150], Loss: 1.9134\n",
      "Epoch [25/150], Loss: 1.8363\n",
      "Epoch [26/150], Loss: 1.8818\n",
      "Epoch [27/150], Loss: 1.8636\n",
      "Epoch [28/150], Loss: 1.7353\n",
      "Epoch [29/150], Loss: 1.8420\n",
      "Epoch [30/150], Loss: 1.8427\n",
      "Epoch [31/150], Loss: 1.7532\n",
      "Epoch [32/150], Loss: 1.7823\n",
      "Epoch [33/150], Loss: 1.7969\n",
      "Epoch [34/150], Loss: 1.8714\n",
      "Epoch [35/150], Loss: 1.7508\n",
      "Epoch [36/150], Loss: 1.8419\n",
      "Epoch [37/150], Loss: 1.7802\n",
      "Epoch [38/150], Loss: 1.7482\n",
      "Epoch [39/150], Loss: 1.7834\n",
      "Epoch [40/150], Loss: 1.7941\n",
      "Epoch [41/150], Loss: 1.8517\n",
      "Epoch [42/150], Loss: 1.7453\n",
      "Epoch [43/150], Loss: 1.7013\n",
      "Epoch [44/150], Loss: 1.7495\n",
      "Epoch [45/150], Loss: 1.7364\n",
      "Epoch [46/150], Loss: 1.8095\n",
      "Epoch [47/150], Loss: 1.7619\n",
      "Epoch [48/150], Loss: 1.7877\n",
      "Epoch [49/150], Loss: 1.8111\n",
      "Epoch [50/150], Loss: 1.7494\n",
      "Epoch [51/150], Loss: 1.8221\n",
      "Epoch [52/150], Loss: 1.8184\n",
      "Epoch [53/150], Loss: 1.7973\n",
      "Epoch [54/150], Loss: 1.8120\n",
      "Epoch [55/150], Loss: 1.7894\n",
      "Epoch [56/150], Loss: 1.8138\n",
      "Epoch [57/150], Loss: 1.8247\n",
      "Epoch [58/150], Loss: 1.7663\n",
      "Epoch [59/150], Loss: 1.7977\n",
      "Epoch [60/150], Loss: 1.7457\n",
      "Epoch [61/150], Loss: 1.7976\n",
      "Epoch [62/150], Loss: 1.7186\n",
      "Epoch [63/150], Loss: 1.8027\n",
      "Epoch [64/150], Loss: 1.7607\n",
      "Epoch [65/150], Loss: 1.8564\n",
      "Epoch [66/150], Loss: 1.7709\n",
      "Epoch [67/150], Loss: 1.8074\n",
      "Epoch [68/150], Loss: 1.7854\n",
      "Epoch [69/150], Loss: 1.7290\n",
      "Epoch [70/150], Loss: 1.7371\n",
      "Epoch [71/150], Loss: 1.7758\n",
      "Epoch [72/150], Loss: 1.7661\n",
      "Epoch [73/150], Loss: 1.6976\n",
      "Epoch [74/150], Loss: 1.7870\n",
      "Epoch [75/150], Loss: 1.6611\n",
      "Epoch [76/150], Loss: 1.7713\n",
      "Epoch [77/150], Loss: 1.7249\n",
      "Epoch [78/150], Loss: 1.6931\n",
      "Epoch [79/150], Loss: 1.6581\n",
      "Epoch [80/150], Loss: 1.7787\n",
      "Epoch [81/150], Loss: 1.7123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/150], Loss: 1.6839\n",
      "Epoch [83/150], Loss: 1.6664\n",
      "Epoch [84/150], Loss: 1.7200\n",
      "Epoch [85/150], Loss: 1.7067\n",
      "Epoch [86/150], Loss: 1.7408\n",
      "Epoch [87/150], Loss: 1.7060\n",
      "Epoch [88/150], Loss: 1.6872\n",
      "Epoch [89/150], Loss: 1.7650\n",
      "Epoch [90/150], Loss: 1.7229\n",
      "Epoch [91/150], Loss: 1.7045\n",
      "Epoch [92/150], Loss: 1.7145\n",
      "Epoch [93/150], Loss: 1.6583\n",
      "Epoch [94/150], Loss: 1.6762\n",
      "Epoch [95/150], Loss: 1.7367\n",
      "Epoch [96/150], Loss: 1.7172\n",
      "Epoch [97/150], Loss: 1.7801\n",
      "Epoch [98/150], Loss: 1.7196\n",
      "Epoch [99/150], Loss: 1.7688\n",
      "Epoch [100/150], Loss: 1.6751\n",
      "Epoch [101/150], Loss: 1.7077\n",
      "Epoch [102/150], Loss: 1.6897\n",
      "Epoch [103/150], Loss: 1.6254\n",
      "Epoch [104/150], Loss: 1.6416\n",
      "Epoch [105/150], Loss: 1.7527\n",
      "Epoch [106/150], Loss: 1.7265\n",
      "Epoch [107/150], Loss: 1.7298\n",
      "Epoch [108/150], Loss: 1.7873\n",
      "Epoch [109/150], Loss: 1.7386\n",
      "Epoch [110/150], Loss: 1.6844\n",
      "Epoch [111/150], Loss: 1.7183\n",
      "Epoch [112/150], Loss: 1.6143\n",
      "Epoch [113/150], Loss: 1.7220\n",
      "Epoch [114/150], Loss: 1.8364\n",
      "Epoch [115/150], Loss: 1.7657\n",
      "Epoch [116/150], Loss: 1.7225\n",
      "Epoch [117/150], Loss: 1.7409\n",
      "Epoch [118/150], Loss: 1.6337\n",
      "Epoch [119/150], Loss: 1.7423\n",
      "Epoch [120/150], Loss: 1.6322\n",
      "Epoch [121/150], Loss: 1.6524\n",
      "Epoch [122/150], Loss: 1.7670\n",
      "Epoch [123/150], Loss: 1.6997\n",
      "Epoch [124/150], Loss: 1.6939\n",
      "Epoch [125/150], Loss: 1.6395\n",
      "Epoch [126/150], Loss: 1.6979\n",
      "Epoch [127/150], Loss: 1.6534\n",
      "Epoch [128/150], Loss: 1.7040\n",
      "Epoch [129/150], Loss: 1.6699\n",
      "Epoch [130/150], Loss: 1.6742\n",
      "Epoch [131/150], Loss: 1.6463\n",
      "Epoch [132/150], Loss: 1.6038\n",
      "Epoch [133/150], Loss: 1.6917\n",
      "Epoch [134/150], Loss: 1.6450\n",
      "Epoch [135/150], Loss: 1.7721\n",
      "Epoch [136/150], Loss: 1.6327\n",
      "Epoch [137/150], Loss: 1.7029\n",
      "Epoch [138/150], Loss: 1.6712\n",
      "Epoch [139/150], Loss: 1.7396\n",
      "Epoch [140/150], Loss: 1.7012\n",
      "Epoch [141/150], Loss: 1.6258\n",
      "Epoch [142/150], Loss: 1.6827\n",
      "Epoch [143/150], Loss: 1.7100\n",
      "Epoch [144/150], Loss: 1.7471\n",
      "Epoch [145/150], Loss: 1.6716\n",
      "Epoch [146/150], Loss: 1.6202\n",
      "Epoch [147/150], Loss: 1.7018\n",
      "Epoch [148/150], Loss: 1.7033\n",
      "Epoch [149/150], Loss: 1.7280\n",
      "Epoch [150/150], Loss: 1.6433\n",
      "The Accuracy of the CNN is: 60.0\n",
      "*********\n",
      "24\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.1509\n",
      "Epoch [2/150], Loss: 2.0777\n",
      "Epoch [3/150], Loss: 1.9447\n",
      "Epoch [4/150], Loss: 1.9624\n",
      "Epoch [5/150], Loss: 1.8261\n",
      "Epoch [6/150], Loss: 1.9053\n",
      "Epoch [7/150], Loss: 1.8969\n",
      "Epoch [8/150], Loss: 1.7527\n",
      "Epoch [9/150], Loss: 1.7870\n",
      "Epoch [10/150], Loss: 1.8880\n",
      "Epoch [11/150], Loss: 1.8461\n",
      "Epoch [12/150], Loss: 1.8170\n",
      "Epoch [13/150], Loss: 1.7251\n",
      "Epoch [14/150], Loss: 1.7445\n",
      "Epoch [15/150], Loss: 1.7362\n",
      "Epoch [16/150], Loss: 1.7230\n",
      "Epoch [17/150], Loss: 1.7223\n",
      "Epoch [18/150], Loss: 1.6832\n",
      "Epoch [19/150], Loss: 1.6187\n",
      "Epoch [20/150], Loss: 1.8159\n",
      "Epoch [21/150], Loss: 1.6286\n",
      "Epoch [22/150], Loss: 1.7093\n",
      "Epoch [23/150], Loss: 1.7787\n",
      "Epoch [24/150], Loss: 1.6693\n",
      "Epoch [25/150], Loss: 1.6742\n",
      "Epoch [26/150], Loss: 1.7411\n",
      "Epoch [27/150], Loss: 1.7064\n",
      "Epoch [28/150], Loss: 1.7436\n",
      "Epoch [29/150], Loss: 1.6672\n",
      "Epoch [30/150], Loss: 1.7261\n",
      "Epoch [31/150], Loss: 1.7475\n",
      "Epoch [32/150], Loss: 1.6041\n",
      "Epoch [33/150], Loss: 1.5820\n",
      "Epoch [34/150], Loss: 1.6594\n",
      "Epoch [35/150], Loss: 1.6974\n",
      "Epoch [36/150], Loss: 1.6871\n",
      "Epoch [37/150], Loss: 1.7389\n",
      "Epoch [38/150], Loss: 1.6867\n",
      "Epoch [39/150], Loss: 1.6308\n",
      "Epoch [40/150], Loss: 1.6775\n",
      "Epoch [41/150], Loss: 1.6368\n",
      "Epoch [42/150], Loss: 1.7836\n",
      "Epoch [43/150], Loss: 1.7108\n",
      "Epoch [44/150], Loss: 1.6021\n",
      "Epoch [45/150], Loss: 1.6047\n",
      "Epoch [46/150], Loss: 1.7055\n",
      "Epoch [47/150], Loss: 1.6776\n",
      "Epoch [48/150], Loss: 1.6190\n",
      "Epoch [49/150], Loss: 1.6519\n",
      "Epoch [50/150], Loss: 1.7323\n",
      "Epoch [51/150], Loss: 1.6799\n",
      "Epoch [52/150], Loss: 1.6061\n",
      "Epoch [53/150], Loss: 1.6743\n",
      "Epoch [54/150], Loss: 1.6013\n",
      "Epoch [55/150], Loss: 1.6284\n",
      "Epoch [56/150], Loss: 1.6617\n",
      "Epoch [57/150], Loss: 1.6535\n",
      "Epoch [58/150], Loss: 1.7133\n",
      "Epoch [59/150], Loss: 1.6959\n",
      "Epoch [60/150], Loss: 1.6459\n",
      "Epoch [61/150], Loss: 1.6585\n",
      "Epoch [62/150], Loss: 1.6215\n",
      "Epoch [63/150], Loss: 1.6731\n",
      "Epoch [64/150], Loss: 1.6817\n",
      "Epoch [65/150], Loss: 1.6541\n",
      "Epoch [66/150], Loss: 1.6442\n",
      "Epoch [67/150], Loss: 1.6742\n",
      "Epoch [68/150], Loss: 1.6166\n",
      "Epoch [69/150], Loss: 1.6439\n",
      "Epoch [70/150], Loss: 1.6251\n",
      "Epoch [71/150], Loss: 1.6415\n",
      "Epoch [72/150], Loss: 1.5910\n",
      "Epoch [73/150], Loss: 1.6722\n",
      "Epoch [74/150], Loss: 1.7081\n",
      "Epoch [75/150], Loss: 1.6510\n",
      "Epoch [76/150], Loss: 1.6500\n",
      "Epoch [77/150], Loss: 1.6401\n",
      "Epoch [78/150], Loss: 1.6486\n",
      "Epoch [79/150], Loss: 1.6923\n",
      "Epoch [80/150], Loss: 1.7443\n",
      "Epoch [81/150], Loss: 1.6679\n",
      "Epoch [82/150], Loss: 1.6703\n",
      "Epoch [83/150], Loss: 1.6342\n",
      "Epoch [84/150], Loss: 1.6553\n",
      "Epoch [85/150], Loss: 1.6462\n",
      "Epoch [86/150], Loss: 1.6506\n",
      "Epoch [87/150], Loss: 1.6456\n",
      "Epoch [88/150], Loss: 1.6183\n",
      "Epoch [89/150], Loss: 1.5691\n",
      "Epoch [90/150], Loss: 1.6131\n",
      "Epoch [91/150], Loss: 1.6132\n",
      "Epoch [92/150], Loss: 1.6816\n",
      "Epoch [93/150], Loss: 1.7842\n",
      "Epoch [94/150], Loss: 1.6783\n",
      "Epoch [95/150], Loss: 1.6964\n",
      "Epoch [96/150], Loss: 1.6605\n",
      "Epoch [97/150], Loss: 1.6532\n",
      "Epoch [98/150], Loss: 1.6286\n",
      "Epoch [99/150], Loss: 1.6257\n",
      "Epoch [100/150], Loss: 1.6065\n",
      "Epoch [101/150], Loss: 1.6392\n",
      "Epoch [102/150], Loss: 1.6613\n",
      "Epoch [103/150], Loss: 1.5950\n",
      "Epoch [104/150], Loss: 1.6217\n",
      "Epoch [105/150], Loss: 1.6849\n",
      "Epoch [106/150], Loss: 1.5493\n",
      "Epoch [107/150], Loss: 1.6638\n",
      "Epoch [108/150], Loss: 1.6173\n",
      "Epoch [109/150], Loss: 1.5599\n",
      "Epoch [110/150], Loss: 1.6541\n",
      "Epoch [111/150], Loss: 1.6035\n",
      "Epoch [112/150], Loss: 1.6272\n",
      "Epoch [113/150], Loss: 1.6355\n",
      "Epoch [114/150], Loss: 1.6374\n",
      "Epoch [115/150], Loss: 1.6367\n",
      "Epoch [116/150], Loss: 1.6756\n",
      "Epoch [117/150], Loss: 1.7933\n",
      "Epoch [118/150], Loss: 1.7141\n",
      "Epoch [119/150], Loss: 1.6243\n",
      "Epoch [120/150], Loss: 1.6710\n",
      "Epoch [121/150], Loss: 1.5763\n",
      "Epoch [122/150], Loss: 1.5528\n",
      "Epoch [123/150], Loss: 1.6939\n",
      "Epoch [124/150], Loss: 1.6186\n",
      "Epoch [125/150], Loss: 1.6558\n",
      "Epoch [126/150], Loss: 1.6642\n",
      "Epoch [127/150], Loss: 1.6879\n",
      "Epoch [128/150], Loss: 1.6241\n",
      "Epoch [129/150], Loss: 1.5878\n",
      "Epoch [130/150], Loss: 1.6233\n",
      "Epoch [131/150], Loss: 1.6088\n",
      "Epoch [132/150], Loss: 1.6800\n",
      "Epoch [133/150], Loss: 1.6555\n",
      "Epoch [134/150], Loss: 1.6214\n",
      "Epoch [135/150], Loss: 1.6085\n",
      "Epoch [136/150], Loss: 1.5815\n",
      "Epoch [137/150], Loss: 1.5688\n",
      "Epoch [138/150], Loss: 1.6294\n",
      "Epoch [139/150], Loss: 1.5999\n",
      "Epoch [140/150], Loss: 1.6260\n",
      "Epoch [141/150], Loss: 1.6436\n",
      "Epoch [142/150], Loss: 1.6550\n",
      "Epoch [143/150], Loss: 1.6985\n",
      "Epoch [144/150], Loss: 1.6636\n",
      "Epoch [145/150], Loss: 1.6591\n",
      "Epoch [146/150], Loss: 1.6470\n",
      "Epoch [147/150], Loss: 1.6169\n",
      "Epoch [148/150], Loss: 1.6469\n",
      "Epoch [149/150], Loss: 1.6196\n",
      "Epoch [150/150], Loss: 1.6194\n",
      "The Accuracy of the CNN is: 75.0\n",
      "*********\n",
      "25\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.1475\n",
      "Epoch [2/150], Loss: 2.0691\n",
      "Epoch [3/150], Loss: 2.0122\n",
      "Epoch [4/150], Loss: 2.0018\n",
      "Epoch [5/150], Loss: 2.0271\n",
      "Epoch [6/150], Loss: 2.0117\n",
      "Epoch [7/150], Loss: 2.0879\n",
      "Epoch [8/150], Loss: 2.0329\n",
      "Epoch [9/150], Loss: 2.0421\n",
      "Epoch [10/150], Loss: 2.0534\n",
      "Epoch [11/150], Loss: 2.0954\n",
      "Epoch [12/150], Loss: 2.0055\n",
      "Epoch [13/150], Loss: 2.0236\n",
      "Epoch [14/150], Loss: 1.9307\n",
      "Epoch [15/150], Loss: 1.9990\n",
      "Epoch [16/150], Loss: 2.0479\n",
      "Epoch [17/150], Loss: 2.0361\n",
      "Epoch [18/150], Loss: 1.9564\n",
      "Epoch [19/150], Loss: 1.9065\n",
      "Epoch [20/150], Loss: 2.0062\n",
      "Epoch [21/150], Loss: 2.0926\n",
      "Epoch [22/150], Loss: 1.9888\n",
      "Epoch [23/150], Loss: 2.0434\n",
      "Epoch [24/150], Loss: 1.9714\n",
      "Epoch [25/150], Loss: 1.9679\n",
      "Epoch [26/150], Loss: 1.9887\n",
      "Epoch [27/150], Loss: 1.9643\n",
      "Epoch [28/150], Loss: 1.9937\n",
      "Epoch [29/150], Loss: 1.8725\n",
      "Epoch [30/150], Loss: 2.0491\n",
      "Epoch [31/150], Loss: 1.9646\n",
      "Epoch [32/150], Loss: 1.9526\n",
      "Epoch [33/150], Loss: 1.9154\n",
      "Epoch [34/150], Loss: 1.9389\n",
      "Epoch [35/150], Loss: 2.0060\n",
      "Epoch [36/150], Loss: 2.0231\n",
      "Epoch [37/150], Loss: 1.9359\n",
      "Epoch [38/150], Loss: 1.9481\n",
      "Epoch [39/150], Loss: 2.0066\n",
      "Epoch [40/150], Loss: 1.9856\n",
      "Epoch [41/150], Loss: 1.9495\n",
      "Epoch [42/150], Loss: 1.9711\n",
      "Epoch [43/150], Loss: 1.9032\n",
      "Epoch [44/150], Loss: 1.9817\n",
      "Epoch [45/150], Loss: 1.9521\n",
      "Epoch [46/150], Loss: 1.8989\n",
      "Epoch [47/150], Loss: 1.9151\n",
      "Epoch [48/150], Loss: 2.0516\n",
      "Epoch [49/150], Loss: 1.8694\n",
      "Epoch [50/150], Loss: 1.9490\n",
      "Epoch [51/150], Loss: 2.0330\n",
      "Epoch [52/150], Loss: 1.8919\n",
      "Epoch [53/150], Loss: 1.9786\n",
      "Epoch [54/150], Loss: 1.9445\n",
      "Epoch [55/150], Loss: 1.9793\n",
      "Epoch [56/150], Loss: 1.8319\n",
      "Epoch [57/150], Loss: 1.8911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/150], Loss: 1.9189\n",
      "Epoch [59/150], Loss: 1.8668\n",
      "Epoch [60/150], Loss: 1.9589\n",
      "Epoch [61/150], Loss: 1.9740\n",
      "Epoch [62/150], Loss: 1.8832\n",
      "Epoch [63/150], Loss: 1.9242\n",
      "Epoch [64/150], Loss: 1.8793\n",
      "Epoch [65/150], Loss: 1.9293\n",
      "Epoch [66/150], Loss: 2.0036\n",
      "Epoch [67/150], Loss: 1.8607\n",
      "Epoch [68/150], Loss: 1.9435\n",
      "Epoch [69/150], Loss: 1.9343\n",
      "Epoch [70/150], Loss: 1.9543\n",
      "Epoch [71/150], Loss: 1.9478\n",
      "Epoch [72/150], Loss: 1.8565\n",
      "Epoch [73/150], Loss: 1.9155\n",
      "Epoch [74/150], Loss: 1.8218\n",
      "Epoch [75/150], Loss: 1.9270\n",
      "Epoch [76/150], Loss: 1.8810\n",
      "Epoch [77/150], Loss: 1.9036\n",
      "Epoch [78/150], Loss: 1.9516\n",
      "Epoch [79/150], Loss: 1.8255\n",
      "Epoch [80/150], Loss: 1.8858\n",
      "Epoch [81/150], Loss: 1.9271\n",
      "Epoch [82/150], Loss: 1.8199\n",
      "Epoch [83/150], Loss: 1.8997\n",
      "Epoch [84/150], Loss: 1.9489\n",
      "Epoch [85/150], Loss: 1.9552\n",
      "Epoch [86/150], Loss: 1.9214\n",
      "Epoch [87/150], Loss: 1.9205\n",
      "Epoch [88/150], Loss: 1.9250\n",
      "Epoch [89/150], Loss: 1.9321\n",
      "Epoch [90/150], Loss: 1.9041\n",
      "Epoch [91/150], Loss: 1.9355\n",
      "Epoch [92/150], Loss: 1.8839\n",
      "Epoch [93/150], Loss: 1.8708\n",
      "Epoch [94/150], Loss: 1.9568\n",
      "Epoch [95/150], Loss: 1.9533\n",
      "Epoch [96/150], Loss: 1.9186\n",
      "Epoch [97/150], Loss: 1.9410\n",
      "Epoch [98/150], Loss: 1.8228\n",
      "Epoch [99/150], Loss: 1.9408\n",
      "Epoch [100/150], Loss: 1.9156\n",
      "Epoch [101/150], Loss: 1.8927\n",
      "Epoch [102/150], Loss: 1.9869\n",
      "Epoch [103/150], Loss: 1.9078\n",
      "Epoch [104/150], Loss: 1.8894\n",
      "Epoch [105/150], Loss: 1.8760\n",
      "Epoch [106/150], Loss: 1.8778\n",
      "Epoch [107/150], Loss: 1.8913\n",
      "Epoch [108/150], Loss: 1.8869\n",
      "Epoch [109/150], Loss: 1.8734\n",
      "Epoch [110/150], Loss: 1.9836\n",
      "Epoch [111/150], Loss: 1.9442\n",
      "Epoch [112/150], Loss: 1.9596\n",
      "Epoch [113/150], Loss: 1.9348\n",
      "Epoch [114/150], Loss: 1.8834\n",
      "Epoch [115/150], Loss: 1.8978\n",
      "Epoch [116/150], Loss: 1.9366\n",
      "Epoch [117/150], Loss: 1.8985\n",
      "Epoch [118/150], Loss: 1.8428\n",
      "Epoch [119/150], Loss: 1.9003\n",
      "Epoch [120/150], Loss: 1.9221\n",
      "Epoch [121/150], Loss: 1.9170\n",
      "Epoch [122/150], Loss: 1.9813\n",
      "Epoch [123/150], Loss: 1.8775\n",
      "Epoch [124/150], Loss: 1.8210\n",
      "Epoch [125/150], Loss: 1.9381\n",
      "Epoch [126/150], Loss: 1.9051\n",
      "Epoch [127/150], Loss: 1.9713\n",
      "Epoch [128/150], Loss: 1.9077\n",
      "Epoch [129/150], Loss: 1.8604\n",
      "Epoch [130/150], Loss: 1.9627\n",
      "Epoch [131/150], Loss: 1.8786\n",
      "Epoch [132/150], Loss: 1.9762\n",
      "Epoch [133/150], Loss: 1.9100\n",
      "Epoch [134/150], Loss: 1.9231\n",
      "Epoch [135/150], Loss: 1.8806\n",
      "Epoch [136/150], Loss: 1.9176\n",
      "Epoch [137/150], Loss: 1.8365\n",
      "Epoch [138/150], Loss: 1.8757\n",
      "Epoch [139/150], Loss: 1.9564\n",
      "Epoch [140/150], Loss: 1.7939\n",
      "Epoch [141/150], Loss: 1.9258\n",
      "Epoch [142/150], Loss: 1.9347\n",
      "Epoch [143/150], Loss: 1.9121\n",
      "Epoch [144/150], Loss: 1.9294\n",
      "Epoch [145/150], Loss: 1.9432\n",
      "Epoch [146/150], Loss: 1.9997\n",
      "Epoch [147/150], Loss: 1.9772\n",
      "Epoch [148/150], Loss: 1.7940\n",
      "Epoch [149/150], Loss: 1.8984\n",
      "Epoch [150/150], Loss: 1.8415\n",
      "The Accuracy of the CNN is: 55.0\n",
      "*********\n",
      "26\n",
      "*********\n",
      "Epoch [1/150], Loss: 2.1596\n",
      "Epoch [2/150], Loss: 2.1598\n",
      "Epoch [3/150], Loss: 2.0620\n",
      "Epoch [4/150], Loss: 2.0698\n",
      "Epoch [5/150], Loss: 2.1324\n",
      "Epoch [6/150], Loss: 2.0008\n",
      "Epoch [7/150], Loss: 2.0005\n",
      "Epoch [8/150], Loss: 2.0288\n",
      "Epoch [9/150], Loss: 2.0293\n",
      "Epoch [10/150], Loss: 1.9636\n",
      "Epoch [11/150], Loss: 2.0320\n",
      "Epoch [12/150], Loss: 2.0222\n",
      "Epoch [13/150], Loss: 1.9051\n",
      "Epoch [14/150], Loss: 1.9459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14844\\1132545365.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[1;31m#train_data, test_data = torch.utils.data.random_split(dataset, [train_set_size, test_set_size])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m                 \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m                 \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2441\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2443\u001b[1;33m     return list(\n\u001b[0m\u001b[0;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[0;32m   2445\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2443\u001b[0m     return list(\n\u001b[0;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[1;32m-> 2445\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2446\u001b[0m         )\n\u001b[0;32m   2447\u001b[0m     )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dies ist die Lernrate, welche wichtig f√ºr die \"Schrittgr√∂sse des Programmes\" ist\n",
    "learning_rate= 0.0000001\n",
    "\n",
    "# Hier ist die Funktion f√ºr das Laden der Daten\n",
    "# Diese Funktion nimmt alle Bilddaten und die Klassen und steckt diese in einen Datensatz \n",
    "# Dieser Datensatz hat dann das Label und die Daten in einem\n",
    "# Diese Daten m√ºssen durch iterationen durch diesen herausgelesen werden\n",
    "class MyDataset(Dataset):\n",
    "            def __init__(self, images, labels):\n",
    "                self.images = images\n",
    "                self.labels = labels\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                image = self.images[index]\n",
    "                label = self.labels[index]\n",
    "                return image, label\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.images)\n",
    "\n",
    "# Definiere das CNN (genauere Erkl√§rung ist weiter oben in dem Dokument)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Definiere die Convolutional Schichten\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Definiere die  max pooling Schichten\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Definiere die fully connected Schichten\n",
    "        self.fc1 = nn.Linear(in_features=576, out_features=128) #64, 128\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "        # Defniere die voll axtiven Funktionen\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Alle funktionen anwenden\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Den output verflachen\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Die voll-verbunden Schichten anwenden\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Die softmax funktion anwenden\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Hier wird der Weg zu den Daten definiert, dieser Variirt mit jedem Nutzer\n",
    "# In dem Ordner, den man dort findet hat es noch verschiedene Unterordner\n",
    "# In diesen Unterordner sind die Daten zu finden\n",
    "folder_path = r'C:\\Users\\41793\\Desktop\\Programming\\BG_Creative_Coding\\rnn_tutorial_npydata_subfolder2'\n",
    "\n",
    "# Hier iterieren wir durch die verschidenen Ordnern\n",
    "# Der ganze nachfolgende Code wird pro Ordner einmal ausgef√ºrt\n",
    "# In meinem Fall hat es in diesem Ordner 35 Unterordner mit jeweils 10 Klassen, mit jeweils ungef√§hr 150'000 Bildern\n",
    "# Dies bedeutet, dass hier der untenstehende Code 35 mal abgerufen wird\n",
    "for folder in os.listdir(folder_path):\n",
    "    \n",
    "    # Dies ist reine kosmetik, so dass ich besser sehen kann bei welchem Ordner wir sind\n",
    "    # Nach dieser Ausgabe des Ordners in welchem wir und befinden, folgen dann sp√§ter die Genauigkeiten des Programmes\n",
    "    print(\"*********\")\n",
    "    print(folder)\n",
    "    print(\"*********\")\n",
    "    \n",
    "    \n",
    "    # Hier erhalten wir den kompletten Weg zu dem Ordner in welchem sich jetzt die 10 Klassen befinden\n",
    "    file_path = os.path.join(folder_path, folder)\n",
    "    \n",
    "    # Hier sehen wir ob der obige ORdnerweg ein weiterer Ordner hat\n",
    "    if os.path.isdir(file_path):\n",
    "        # Falls dies der Fall ist, soll durch diese Daten durch iteriert werden\n",
    "        for subfile in os.listdir(file_path):\n",
    "            # Hier erhalten wir den vollen Weg zu jenem Unterordner\n",
    "            subfile_path = os.path.join(file_path, subfile)\n",
    "            \n",
    "        # Jetzt sagen wir, dass die Daten welche in diesem Ordner sind files heissen, solange sie mit .npy enden\n",
    "        # Hier nutzen wir die glob Library welche es uns erm√∂glicht einfacher auf Ordner auf underem Ger√§t zuzugreifen\n",
    "        files = glob.glob(os.path.join(file_path, \"*.npy\"))\n",
    "\n",
    "        # Nun setzten wir leere Listen f√ºr die Daten und Labels fest\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "        label_list_alone = []\n",
    "        i = 0\n",
    "        # Hier gehe wir √ºber die verschiedenen Dateien in Dateien\n",
    "        for file in files:\n",
    "            # Nun legen wir das Label fest\n",
    "            # Da dies noch nicht in den Daten vorgegeben wurden, nehmen wir einfach den Namen der Datei\n",
    "            # Dies erreichen wir wieder mit der glob Library, indem wir einfach nur den Basename nehmen\n",
    "            # Dieser Basename ist somit der einfache Name der Datei, ohne alle Ordner, welche davor noch k√§men\n",
    "            # Wir spalten also den String bei diesem Basename und nehmen dann nur den teil in welchem der Name der Datei ist\n",
    "            label = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "            # Hier √∂ffnen wir die Daten in Binarymode\n",
    "            # Wir k√∂nnen die Daten nur mit dem rb = read in binary √∂ffnen\n",
    "            # Dies ist der Fall, da man nicht wie normal die Daten mit r laden kann, wegen ihres Formates\n",
    "            # Passen sie hier auf, dass sie die Daten nicht mit wb √∂ffnen,da die Daten sonst automatisch √ºberschrieben werden\n",
    "            # In underem Fall w√§re dies ung√ºnstig, da wir nichts reinschreiben und die daten somit leer werden\n",
    "            with open(file, 'rb') as fin:\n",
    "                # Nun werden die Daten mit der Library Numpy geladen\n",
    "                data = np.load(fin)\n",
    "                # Da wir nur ein Label pro Datei haben m√ºssen wir dieses vervielf√§tigen\n",
    "                # Falls wir dies nicht tun, haben wir das Problem, dass die Daten-liste und Label-Liste nicht gleich lang ist\n",
    "                # Wir l√∂sen dies, indem wir einfach f√ºr die l√§nge der Daten das Label der Label-Liste hinzuf√ºgen\n",
    "                for i in range (0, len(data)):\n",
    "                    label_list.append(label)\n",
    "            # Hier f√ºgen wir nun auch die Daten der Daten-Liste hinzu\n",
    "            data_list.append(data)\n",
    "         \n",
    "        # Dies ist die Gr√∂sse, mit welcher die Daten geladen werden\n",
    "        batch_size = 100\n",
    "\n",
    "        # Concatenate data and labels into arrays\n",
    "        # Nun verketten wir die Daten und Labels zu Reihen, hier Array genannt\n",
    "        # Die Daten werden hier auf eine Achse beschr√§nkt, damit sie die gleichen Dimensionen wie die Lables haben\n",
    "        data = np.concatenate(data_list, axis=0)\n",
    "        labels = np.array(label_list)\n",
    "\n",
    "        # Jetzt machen wir aus den Daten Kommazahlen, da eine sp√§tere Funktionen nicht mit Integerns funktioniert\n",
    "        data = data.astype(float)\n",
    "        \n",
    "        # Hier machen wir ein W√∂rterbuch f√ºr die Lables\n",
    "        # Dies wird hier ben√∂tigt, da mein CNN nur Zahlen als Input nehmen kann\n",
    "        # Das heisst, dass die Lables auch in Zahlenform sein m√ºssen, damit eine Evaluation statt finden kann\n",
    "        # Dieses W√∂rtbuch hat nun zumbeispiel den Eintrag: \"Auto = 1, Biene = 2, etc.\"\n",
    "        label_dict = {label: i for i, label in enumerate(set(labels))}\n",
    "\n",
    "        # Convert labels to integers using the dictionary\n",
    "        # Nun verwandeln wir die Lables, welche noch Strings sind, zu Integers\n",
    "        # Dies erreichen wir, indem wir das label_dict verwenden\n",
    "        labels = np.array([label_dict[label] for label in labels])\n",
    "\n",
    "        # Dies ist mehr eine absicherung, dass auch wirklch alles eine Array ist\n",
    "        data = np.array(data)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Hier sehen wir definieren wir, ob die Daten farbig oder schwarz, weiss sind\n",
    "        # Falls num_channels = 3 sind, dann w√§re es farbig, aufgrund der drei rgb farben\n",
    "        # Falls jedoch, wie in unserem Fall dies 1 betr√§gt sind die Daten Schwarz-Weiss\n",
    "        num_channels = data.ndim - 1\n",
    "        \n",
    "        \"\"\"\n",
    "        # Hier k√∂nnte man noch einige Bilder ausgebe lassen, falls man eine Idee haben will, wie die Daten aussehen\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        for i in range(1):\n",
    "            sample = data[i]\n",
    "            print(f'Sample {i}: {sample}')\n",
    "\n",
    "        # Wir nehmen an, das die Daten eine 2D Reihe sind, mit Form (h√∂che, breite)\n",
    "            image = np.array(sample)\n",
    "            image = image.reshape(28, 28)\n",
    "\n",
    "            # Zeig das Bild\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "            \"\"\"\n",
    "\n",
    "        # Hier sagen wir, das 80% der Batchsize die Input Batchsize ist\n",
    "        # Dies wirdvnoch sp√§ter verwendet, um die Menge der Daten zu bestimmen\n",
    "        input_batch = int(0.8* batch_size)\n",
    "\n",
    "        # Diese Funktion h√§tte direkt im CNN defniert werden k√∂nnen\n",
    "        # Ich habe sie hier definiert, da ich es erst w√§hrend des schon laufenden Programmes bemerkte\n",
    "        # Es mach keinen unterschied wo es definiert wird\n",
    "        output_features = 24 * 24\n",
    "\n",
    "        # W√§hend des Testes hatte ich mehr mals den Fehler wegen des Types\n",
    "        # Wegen dem habe ich die Daten hier noch einmal, ist jedoch nicht n√∂tig\n",
    "        # Da dieser Code hier nur 35 Mal ablauft, hat dies keinen grossen einfluss auf die Effizienz\n",
    "        data = np.array(data)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Hier w√ºrden die Daten auf der CPU geladen werden, falls die Grafikkarte zu schwach w√§re, oder man keine hat\n",
    "        #data = data.cpu().numpy()\n",
    "        #labels = labels.cpu().numpy()\n",
    "\n",
    "        # Create a dataset from your data and labels\n",
    "        # Nun kreiiren wir ein Datenset mit unseren Daten und Labels\n",
    "        # Man greift auf die zu Beginn defnierte Funktion MyDataset zur√ºck \n",
    "        dataset = MyDataset(data, labels)\n",
    "\n",
    "        # Hier definieren wir noch einige notwendige Variablen f√ºr den Rest des Programmes \n",
    "        input_size = data.shape[1]\n",
    "        hidden_size = 128\n",
    "        num_layers = 1\n",
    "        num_classes = len(label_dict)\n",
    "        batch_size = batch_size  \n",
    "\n",
    "        # Jetzt wird das CNN inizialiesiert, dass heisst geladen\n",
    "        # Die num_classes sind die Anzahl an Klassen, welche wir haben, dies kann aus dem W√∂rterbuch herausgelsesen werden\n",
    "        cnn = CNN(num_classes=num_classes)\n",
    "\n",
    "        # Da das CNN in unserem Fall sehr viel Nechenleistung beanspruch m√ºssen wir es wenn m√∂glich verschieben\n",
    "        # Dies erreichen wir, indem wir mit torch.device nachschauen was vorhanden ist\n",
    "        # F√ºr den Fall, das eine Grafikkarte vorhanden ist und Cuda unterst√ºtzt wird, ist device = GPU\n",
    "        # Dies ist stark zu empfehlen, da die Rechenzeit sonst viel zu hoch ist\n",
    "        # F√ºr den Fall das keine GPU vorhande ist, wird einfach die CPU verwendet\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Hier wird das CNN zu dem verf√ºgbaren Ger√§t verschoben\n",
    "        cnn.to(device)\n",
    "\n",
    "        # Nun definieren wir die Loss und Optimizer funktionen\n",
    "        # Diese sind essenziell f√ºr das CNN, da es sonst nicht lernen k√∂nnte\n",
    "        # Die CrossEntropyLoss Funktion ist eine h√§ufig verwendete Verlust-Funktion\n",
    "        # Es misst den unterschied zischen der Vorhergesagten Verteilung der Klassen und der realen Verteilung\n",
    "        # Dies wird vereinfaacht berechnet, in dem man den unterschied nehmen und diesen dann mit -y * ln(y) multiplizieren\n",
    "        # Hier wird dies ausf√ºhrlich erkl√§rt: https://vitalflux.com/mean-squared-error-vs-cross-entropy-loss-function/\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # Nun haben wir hier den optimizer\n",
    "        # Hier wurde der Adam optimizer gew√§hlt, welcher einer der h√§ufigsten ist\n",
    "        # Die Wahl wurde auf Grund von Tests genommen\n",
    "        # In einem CNN (Convolutional Neural Network) braucht man einen Optimierer, um die Gewichte des Netzwerks zu aktualisieren und zu optimieren. Der Optimierer ist daf√ºr verantwortlich, den Fehler des Netzwerks zu minimieren, indem er die Gewichte des Netzwerks anpasst, um die Vorhersagen des Netzwerks so nah wie m√∂glich an den tats√§chlichen Ergebnissen zu bringen.\n",
    "        # Ein Optimierer verwendet die Gradienten des Verlustes, die durch die Backpropagation berechnet werden, um die Gewichte des Netzwerks zu aktualisieren. Der Gradient beschreibt die √Ñnderung des Verlustes in Bezug auf die Gewichte und die Optimierer nutzen diese Informationen, um die Gewichte zu aktualisieren und den Verlust zu minimieren.\n",
    "        # Der Adam Optimierer kombiniert die Vorteile von zwei anderen Optimierern: dem Momentum Optimierer und dem AdaGrad Optimierer.\n",
    "        # Der Momentum Optimierer hilft dabei, das Netzwerk schneller aus flachen Minima herauszubewegen und das AdaGrad Optimierer hilft dabei, die Lernrate f√ºr jeden Parameter anzupassen.\n",
    "        # Adam nutzt diese zwei ans√§tze und ist somit popul√§r\n",
    "        # Fr√º eine genauere erkl√§rung verweise ich gerne auf diesen Artikel: https://artemoppermann.com/de/optimierung-in-deep-learning-adagrad-rmsprop-adam/\n",
    "        optimizer = torch.optim.Adam(cnn.parameters(), lr = learning_rate)\n",
    "\n",
    "        test_batch = int(batch_size * 0.2)\n",
    "\n",
    "        # Nun laden wir die Daten mit einer der vielen Funktionen von PyTorch\n",
    "        # Diese Funktion erstellt einen Dataloader, der die Daten in Batches aufteilt und sie zuf√§llig mischt (shuffle=True) und das letzte Batch verwirft (drop_last=True), wenn es nicht vollst√§ndig gef√ºllt ist.\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        # Hier teile ich die Daten in Training und Test Daten\n",
    "        train_set_size = int(len(dataset) * 0.8)\n",
    "        test_set_size = len(dataset) - train_set_size\n",
    "\n",
    "        # Nun definieren wir die Anzahl an Epochen f√ºr welches das CNN trainieren wird\n",
    "        # Ich hatte diesen Wert meistens bei 125 um das Netz zu trainieren\n",
    "        # Dies f√ºrte dazu, dass ich meinen PC  f√ºr eine ganze Woche laufen lies\n",
    "        # Die Trainingszweit betrug 150 Stunden, wobei die Resultate mit l√§ngerem lernen noch besser werden w√ºrden\n",
    "        num_epochs = 150\n",
    "        # Hier folgt eine for Schleife, welche f√ºr die Anzahl an Epochen ausgef√ºrt wird\n",
    "        for epoch in range(num_epochs):\n",
    "            # Jetzt Iterieren wir √ºber den Dataloader, der die Daten in Batches aufteilt. \n",
    "            for data, labels in dataloader:\n",
    "                # Wir wandeln die Labels in den Datentyp LongTensor\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                # Hier verschieben wir die Daten dann auf die GPU durch die Funktion .cuda()\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "                # Wir splitten dann die Daten in Trainings- und Testdaten mit dem train_test_split() von sklearn\n",
    "                train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2)\n",
    "                # Wir wandeln die Trainingsdaten in den Datentyp float um √§ndern die Form der Daten von (batch_size, 28, 28) in (batch_size, 1, 28, 28)\n",
    "                train_data = train_data.to(dtype=torch.float)\n",
    "                # Danach √§ndern wir die Form der Daten von (batch_size, 28, 28) in (batch_size, 1, 28, 28)\n",
    "                train_data = train_data.view(-1, 28, 28)\n",
    "                train_data = train_data.view(input_batch, 1, 28, 28)\n",
    "                # Wir f√ºhren die Daten durch das CNN und speichern das Ergebnis in der Variablen \"output\".\n",
    "                # Dies ist der Schritt, welcher bei weitem am meisten Fehelr verursacht hatte\n",
    "                # Dies ist so, da der Code bis hier hin ohne Fehler gelaufen ist, jedoch dann immer ein Fehler in Bezug auf die Form hervor kam\n",
    "                # Dieser Schritt braucht als einzige Variabel wahrscheinlich am meisten Zeit\n",
    "                # Zu Grunde liegen die vielen Umwandlung welche in unserem CNN geschehen\n",
    "                output = cnn(train_data)\n",
    "                # Wir berechnen den Verlust (loss) mit der verwendeten Loss-Funktion (criterion) zwischen dem output des Netzes und den tats√§chlichen Trainingslabels.\n",
    "                # Dies geschieht einfach indem wir die zufor definierte Funktion aufrufen\n",
    "                loss = criterion(output, train_labels)\n",
    "                # Wir setzen die Gradienten auf Null (zero_grad()) \n",
    "                optimizer.zero_grad()\n",
    "                # Nun f√ºhren einen Backpropagation Schritt aus (backward())\n",
    "                loss.backward()\n",
    "                # Zu letzt aktualisieren wir die Gewichte des Netzes mit dem verwendeten Optimierer (optimizer.step()).\n",
    "                optimizer.step()\n",
    "                \n",
    "            # Wenn die aktuelle Epoche ein Vielfaches von 1 ist, geben wir den aktuellen Verlust aus.\n",
    "            # Mit den Werten welche ich gew√§hlt hatte, passiert dies alle 100 Sekunden\n",
    "            if (epoch+1) % 1 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                \n",
    "        # Dieser Code stellt eine Schleife dar, die verwendet wird, um die Genauigkeit des trainierten CNNs auf einem Testdatensatz zu √ºberpr√ºfen\n",
    "        # Nun setzen wir das CNN zu Evaluations-Modus, dies bedeutet, dass das CNN nicht trainiert wird\n",
    "        cnn.eval() \n",
    "        # Hier schalten wir die gradient Optimisierung ab\n",
    "        with torch.no_grad():\n",
    "            # Nun deklarieren wir die Variabeln correct und total das erste mal\n",
    "            # Wir setzen diese auf null, da dies der nat√ºrliche Ausgangspunkt ist\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Nun iterieren wir genau glecih wie oben durch die Daten und verwenden die Testdaten \n",
    "            # Hier trainieren wird das Model jedoch nicht, sondern schauen einfach, was die Gewichte ausgeben w√ºrden\n",
    "            for data, labels in dataloader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "                train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2)\n",
    "                test_data = test_data.to(dtype=torch.float)\n",
    "                test_data = test_data.view(-1, 28, 28)\n",
    "                test_data = test_data.view(test_batch, 1, 28, 28)\n",
    "                # Hier werden die Daten wieder durch das CNN gegeben und es gibt uns somit die Vorhersage zur√ºck\n",
    "                outputs = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Berechne die Anzahl an richtigen Vorhersagen\n",
    "                correct = (predicted == test_labels).sum().item()\n",
    "\n",
    "                # Nun berechnen wir in Prozent wie genau unser Model ist\n",
    "                accuracy = 100 * correct / test_labels.size(0)\n",
    "            # Nun geben wir die Genauigkeit aus.\n",
    "            # Dies geschieht am Ende jedes Ordners und dies ist auch die Genauigkeit mit welcher das Model neue Bilder richtig zuorden wird\n",
    "            # Diese Genauigkeit betr√§gt bei meinem Training im Schnitt um die 75%\n",
    "            print(\"The Accuracy of the CNN is:\", accuracy)\n",
    "\n",
    "        # Damit wir dieses Model wieder verwenden k√∂nnen, m√ºssen wir die Gewichte des Models in einem .pt speichern\n",
    "        # Um zu verhindern, dass wir die schon zu vor gespeicherten Werte √ºberschreiben, nehmen wir die √§ndernde Variable folder in den Namen der Datei\n",
    "        # Durch diesen Namen ist es auch einfacher wieder sicherzustellen zu welchem Subfolder die Gewichte geh√∂ren\n",
    "        torch.save(cnn.state_dict(), 'cnn_{}.pt'.format(folder))\n",
    "        \n",
    "        # Mir steht nur eine gewisse Anzahl an Speicher zur verf√ºgung\n",
    "        # Auf Grund dessen, muss ich die Variabel data l√∂schen\n",
    "        # Diese Variabel beansprucht in meinem Fall 10 GB an RAM und auch noch alle 8 Gb VRAM meiner GPU\n",
    "        del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(cnn.state_dict(), 'cnn_a_long.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166591d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
